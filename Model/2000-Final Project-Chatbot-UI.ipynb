{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impossible-knitting",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "engaging-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-requirement",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smooth-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('intent-classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "institutional-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    # Reading the custom excel file designed for DURHAM COLLEGE COVID AWARENESS\n",
    "    df = pd.read_excel(filename, names = ['Questions', 'Intents', 'Answer'])\n",
    "    # Extract the Intents, Answers, and Questions\n",
    "    intent = df['Intents']\n",
    "    answer = df['Answer']\n",
    "    # Extract unique Intents and Answers and zip them in Key-Value Pairs\n",
    "    unique_intent = list(set(intent))\n",
    "    unique_answers = list(set(answer))\n",
    "    intent_answer_dict = dict(zip(unique_intent, unique_answers))\n",
    "    questions = list(df['Questions'])\n",
    "\n",
    "    return (intent, unique_intent, questions, intent_answer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "physical-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent, unique_intent, sentences, intent_answer_dict = load_dataset('covid-intents.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-collar",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recovered-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(sentences):\n",
    "    words = []\n",
    "    # Extract the sentences \n",
    "    for s in sentences:\n",
    "        # Remove the punctuations\n",
    "        clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "        # Extract the words from the sentence\n",
    "        w = word_tokenize(clean)\n",
    "        # Converting the words to Lower case and converting them back to a list\n",
    "        words.append([i.lower() for i in w])\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "frank-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_words = cleaning(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "shared-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
    "    # Create a tokenizer with the filters defined above\n",
    "    token = Tokenizer(filters = filters)\n",
    "    # Fit the created tokenizer on the sentences\n",
    "    token.fit_on_texts(words)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "after-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(words):\n",
    "    # Get the Maximum length of all the sentences\n",
    "    return(len(max(words, key = len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "southeast-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = create_tokenizer(cleaned_words)\n",
    "vocab_size = len(word_tokenizer.word_index) + 1\n",
    "max_length = max_length(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "promotional-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_doc(token, words):\n",
    "    # Encoding the list of words to list of integers based on BoW\n",
    "    return(token.texts_to_sequences(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "interpreted-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the sentences\n",
    "encoded_doc = encoding_doc(word_tokenizer, cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "oriental-corporation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_doc(encoded_doc, max_length):\n",
    "    # Padding the sequence of integers to max-len of sentences\n",
    "    return(pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "neither-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the Sequences\n",
    "padded_doc = padding_doc(encoded_doc, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "decent-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Preprocess the manual user inputs\n",
    "def predictions(text):\n",
    "    # Remove Punctuations\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n",
    "    # Extract the words from the sentence\n",
    "    test_word = word_tokenize(clean)\n",
    "    # Converting the words to Lower case and converting them back to a list\n",
    "    test_word = [w.lower() for w in test_word]\n",
    "    # Encode sentences to sequences\n",
    "    test_ls = word_tokenizer.texts_to_sequences(test_word)\n",
    "    print(test_word)\n",
    "    # Check for unknown words not in the actual BoW\n",
    "    if [] in test_ls:\n",
    "        test_ls = list(filter(None, test_ls))\n",
    "\n",
    "    test_ls = np.array(test_ls).reshape(1, len(test_ls))\n",
    "    # Pad the sequences to max length\n",
    "    x = padding_doc(test_ls, max_length)\n",
    "    # Predict the Intent\n",
    "    pred = model.predict_proba(x)\n",
    "\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "norman-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_output(pred, classes):\n",
    "    # Get the Prediction scores\n",
    "    predictions = pred[0]\n",
    "    # Get all the unique Intents\n",
    "    classes = np.array(classes)\n",
    "    # Sort the prediction scores for all Intents\n",
    "    ids = np.argsort(-predictions)\n",
    "    # Sort all the Intents according to the scores using sorted indexes\n",
    "    classes = classes[ids]\n",
    "    predictions = -np.sort(-predictions)\n",
    "    # Select the Top Intent\n",
    "    predicted_intent = classes[0]\n",
    "    # Select the Answer for the Intent selected (Key-value pair)\n",
    "    return intent_answer_dict[predicted_intent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rotary-replacement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'would', 'like', 'to', 'get', 'some', 'technical', 'assistance']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rosha\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once COVID AWARENESS App is set up for your Id, all you need to do scan the code which takes them to a unique page for your business. They then submit their contact information and answer the screening questions based on local legislation. Once completed they receive a green check mark to show you they have completed the process.\n"
     ]
    }
   ],
   "source": [
    "text = \"I would like to get some technical assistance.\"\n",
    "pred = predictions(text)\n",
    "answer = get_final_output(pred, unique_intent)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-enclosure",
   "metadata": {},
   "source": [
    "### Chatbot UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "joint-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "intense-capture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'i', 'need', 'some', 'information']\n",
      "['what', 'is', 'contact', 'tracing']\n",
      "['hey', 'how', 'are', 'you']\n"
     ]
    }
   ],
   "source": [
    "def send():\n",
    "    msg = EntryBox.get(\"1.0\",'end-1c').strip()\n",
    "    EntryBox.delete(\"0.0\",END)\n",
    "    # Adding Msg to screen, Clearing the Msg Box and Adding the reply to the chatbot from Model used for Prediction\n",
    "    if msg != '':\n",
    "        ChatLog.config(state=NORMAL)\n",
    "        ChatLog.insert(END, \"You: \" + msg + '\\n\\n')\n",
    "        ChatLog.config(foreground=\"#442265\", font=(\"Verdana\", 12 ))\n",
    "\n",
    "        pred = predictions(msg)\n",
    "        res = get_final_output(pred, unique_intent)\n",
    "        \n",
    "        ChatLog.insert(END, \"Bot: \" + res + '\\n\\n')\n",
    "\n",
    "        ChatLog.config(state=DISABLED)\n",
    "        ChatLog.yview(END)\n",
    "\n",
    "\n",
    "base = Tk()\n",
    "# Chat Window Dimensions\n",
    "base.title(\"Durham College COVID-19 Assistance - Chat Bot\")\n",
    "base.geometry(\"400x500\")\n",
    "base.resizable(width=FALSE, height=FALSE)\n",
    "\n",
    "# Creating Chat window\n",
    "ChatLog = Text(base, bd=0, bg=\"white\", height=\"8\", width=\"50\", font=\"Arial\",)\n",
    "\n",
    "ChatLog.config(state=DISABLED)\n",
    "\n",
    "# Adding scrollbar to Chat window\n",
    "scrollbar = Scrollbar(base, command=ChatLog.yview, cursor=\"heart\")\n",
    "ChatLog['yscrollcommand'] = scrollbar.set\n",
    "\n",
    "# Creating Button to send message\n",
    "SendButton = Button(base, font=(\"Verdana\",12,'bold'), text=\"Send\", width=\"12\", height=5,\n",
    "                    bd=0, bg=\"#33AAFF\", activebackground=\"#3c9d9b\",fg='#ffffff',\n",
    "                    command= send )\n",
    "\n",
    "# Creating the message-box to enter a message\n",
    "EntryBox = Text(base, bd=0, bg=\"white\",width=\"29\", height=\"5\", font=\"Arial\")\n",
    "\n",
    "\n",
    "# Adding all components on the screen\n",
    "scrollbar.place(x=376,y=6, height=386)\n",
    "ChatLog.place(x=6,y=6, height=386, width=370)\n",
    "EntryBox.place(x=128, y=401, height=90, width=265)\n",
    "SendButton.place(x=6, y=401, height=90)\n",
    "\n",
    "base.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-daniel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
